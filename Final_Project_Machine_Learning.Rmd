---
title: 'Final Project Machine Learning'
author: "Geovanni Becerril"
date: "2018-02-15"
output: html_document
---

#Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

#Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

```{r echo=FALSE}
setwd("C:/Users/demyc 13/Documents/Geovanni/Coursera (Data Science)/8. Practical Machine Learning/Project")
```

Read the data, previously downloaded:

```{r}
training_complete<-read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!",""))
testing_complete<-read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!",""))
```

#Cleaning the data sets

The data sets have a lot NA's. so it was decided to keep the variables that had complete information and don't have unique value.

```{r,warning=FALSE}
#Loading libraries
library(caret)
library(randomForest)
library(rpart)
#Eliminate the predictors that have unique value.
unique.value<-nearZeroVar(training_complete)
training_aux<-training_complete[,-c(1,unique.value)]
#Keep columns with complete data
training<-training_aux[,c(2,3,5:9,32:44,46:54,65:67,80,91:102,112,114:123)]
```

#Data partitioning

The training dataset contains the 75% of data, while the testing set, to evaluate the model, contains the 25%.

```{r}
set.seed(11235)
inTrain<-createDataPartition(training$classe,p=0.75,list=FALSE)
training_set<-training[inTrain,]
testing_set<-training[-inTrain,]
```

The training dataset contains 14,718 observations. The testing dataset contains 4,904 observations.

#Fitting the model

We use three different machine learning algorithms to select the best model to predict: Gradient Boosted Machine, Random Forest and Decision Trees.

1. Gradient Boosted Machine (GBM)

The code used is the following:

```{r,echo=FALSE,include=FALSE}
set.seed(11235)
fitGBM<-train(classe~.,data=training_set,method="gbm",
               trControl=trainControl(method="repeatedcv",
                                      number=5,repeats=1))
pred_GBM<-predict(fitGBM,newdata = testing_set)
cm_GBM<-confusionMatrix(pred_GBM, testing_set$classe)
```

```{r,eval=FALSE}
set.seed(11235)
fitGBM<-train(classe~.,data=training_set,method="gbm",
               trControl=trainControl(method="repeatedcv",
                                      number=5,repeats=1))
pred_GBM<-predict(fitGBM,newdata = testing_set)
cm_GBM<-confusionMatrix(pred_GBM, testing_set$classe)
```

The confusion matrix is:

```{r}
cm_GBM
```

2. Random Forests (RF)

The code used is the following:

```{r}
set.seed(11235)
fitRF<-randomForest(classe ~ ., data=training_set)
pred_RF<-predict(fitRF, testing_set, type = "class")
cm_RF<-confusionMatrix(pred_RF, testing_set$classe)
cm_RF
```

3. Decision Trees (DT)

The code used is the following

```{r}
set.seed(11235)
fitDT<-rpart(classe~.,data=training_set,method="class")
pred_DT<-predict(fitDT, testing_set, type = "class")
cm_DT<-confusionMatrix(pred_DT, testing_set$classe)
cm_DT
```

#Discussion

In the confusionMatrix it is observed that the algorithm with greater precision is the GBM and the RF, while the DT is bad for predicting the data. The accuracy obtained in each algorithm can be seen in the following table:

Algorithm|Accuracy
---------|--------
Gradient Boosted Machine|`r as.numeric(cm_GBM$overall[1])`
Random Forests|`r as.numeric(cm_RF$overall[1])`
Decision Trees|`r as.numeric(cm_DT$overall[1])`

#Predicting in the Testing Data

The goal is to use the prediction model to predict 20 different test cases. the algorithm to use is the Random Forest because it has better accuracy. The following code makes the prediction for the 20 test cases.

```{r}
predict_final<-predict(fitRF,newdata = testing_complete)
```

The predictions for the 20 test case are:

```{r}
predictions<-data.frame(names=testing_complete[,2],class=predict_final)
predictions
```
